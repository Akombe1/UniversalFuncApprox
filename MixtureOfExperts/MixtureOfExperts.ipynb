{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of Mixture of Experts Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1. Create Synthetic Dataset\n",
    "# ------------------------------\n",
    "# We'll create a simple 1D regression problem. For example:\n",
    "#   y = sin(x) + 0.2 * noise\n",
    "#   We'll make x in [-2π, 2π] to see interesting behavior.\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "N = 200\n",
    "X = np.linspace(-2*np.pi, 2*np.pi, N)\n",
    "Y = np.sin(X) + 0.2 * np.random.randn(N)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_torch = torch.tensor(X, dtype=torch.float).view(-1, 1)\n",
    "Y_torch = torch.tensor(Y, dtype=torch.float).view(-1, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 2. Define Expert Networks\n",
    "# ------------------------------\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=16, output_dim=1):\n",
    "        super(Expert, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Define Gating Network\n",
    "# ------------------------------\n",
    "\n",
    "class GatingNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=1, num_experts=3):\n",
    "        super(GatingNetwork, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_experts)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Output a distribution over experts using softmax\n",
    "        logits = self.linear(x) \n",
    "        return torch.softmax(logits, dim=-1)\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Mixture of Experts Model\n",
    "# ------------------------------\n",
    "\n",
    "class MixtureOfExperts(nn.Module):\n",
    "    def __init__(self, num_experts=3, input_dim=1, hidden_dim=16, output_dim=1):\n",
    "        super(MixtureOfExperts, self).__init__()\n",
    "        \n",
    "        # Create N experts\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(input_dim, hidden_dim, output_dim) for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "        # Create gating network\n",
    "        self.gating_network = GatingNetwork(input_dim, num_experts)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get gating probabilities\n",
    "        gating_probs = self.gating_network(x)  # shape: (batch_size, num_experts)\n",
    "        \n",
    "        # Get expert outputs\n",
    "        expert_outputs = []\n",
    "        for expert in self.experts:\n",
    "            expert_outputs.append(expert(x))  # shape: (batch_size, 1)\n",
    "        \n",
    "        # Stack expert outputs -> (batch_size, num_experts, 1)\n",
    "        expert_outputs = torch.stack(expert_outputs, dim=1).squeeze(-1)\n",
    "        # expert_outputs shape is (batch_size, num_experts)\n",
    "        \n",
    "        # Weighted sum across experts\n",
    "        # gating_probs: (batch_size, num_experts)\n",
    "        # expert_outputs: (batch_size, num_experts)\n",
    "        # => mixture: (batch_size, )\n",
    "        mixture_output = torch.sum(gating_probs * expert_outputs, dim=1, keepdim=True)\n",
    "        \n",
    "        return mixture_output, gating_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 5. Training the MoE model\n",
    "# ------------------------------\n",
    "\n",
    "num_experts = 3\n",
    "model = MixtureOfExperts(num_experts=num_experts, input_dim=1, hidden_dim=16, output_dim=1)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "num_epochs = 2000\n",
    "\n",
    "X_train = X_torch\n",
    "Y_train = Y_torch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs, gating_probs = model(X_train)\n",
    "    loss = criterion(outputs, Y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 500 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 6. Plot Results\n",
    "# ------------------------------\n",
    "\n",
    "# Evaluate model on a dense set of points for plotting\n",
    "with torch.no_grad():\n",
    "    X_plot = torch.linspace(-2*np.pi, 2*np.pi, 300).view(-1,1)\n",
    "    y_pred, gating_pred = model(X_plot)\n",
    "    y_pred = y_pred.detach().numpy()\n",
    "    gating_pred = gating_pred.detach().numpy()\n",
    "\n",
    "# Plot the learned gating probabilities\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Subplot 1: Mixture output\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.scatter(X, Y, label=\"Data\", s=20, alpha=0.6)\n",
    "plt.plot(X_plot.numpy(), y_pred, 'r-', label=\"MoE Prediction\", linewidth=2)\n",
    "plt.title(\"Mixture-of-Experts Regression\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: Gating probabilities for each expert\n",
    "for i in range(num_experts):\n",
    "    plt.plot(X_plot.numpy(), gating_pred[:, i], label=f\"Gating Expert {i}\")\n",
    "plt.title(\"Gating Network Output\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
