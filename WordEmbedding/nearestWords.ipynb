{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest words via embeddings\n",
    "Here is the prompt to generate this code in ChatGPT\n",
    "Write a python program using pytorch to create embeddings for a list of words. Then given the input of one of the words find the 5 closest words to it. \n",
    "\n",
    "Question - Are the embeddings normalized? ie are they of length 1 ?\n",
    "\n",
    "## Exercise in Class\n",
    "Add a capability to embed 5 phrases containing up to 30 words, \n",
    "then given a query select the phrase that might answer that query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.nn' from '/home/codespace/.local/lib/python3.12/site-packages/torch/nn/__init__.py'>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings for the first two words:\n",
      "tensor([[-0.3587,  0.7732, -0.4567, -0.4940, -0.0791, -0.0285,  0.7708,  0.8347,\n",
      "         -1.5237,  1.0784, -1.8059,  0.6040, -1.5777,  0.1740,  1.4177,  0.9748,\n",
      "         -1.6658,  1.6476,  0.0658,  0.5767,  0.1060,  1.3074,  0.9831,  0.3646,\n",
      "          0.4986, -1.0046,  0.9260,  0.4735, -0.0531, -1.0035, -0.8946, -0.2291,\n",
      "         -0.5259, -0.3352,  0.6604,  1.0024, -0.1573, -2.2002,  0.3426, -0.3788,\n",
      "          0.7011,  1.0688, -0.5525,  0.4026, -0.0403,  0.0140,  0.5893, -0.0812,\n",
      "         -1.1798, -0.3190],\n",
      "        [-0.3961,  1.5104, -0.8098,  0.9906,  0.9506, -1.0023, -0.8107, -1.0364,\n",
      "          0.1094, -0.6319, -1.2142, -0.3318, -0.1119,  0.8757,  0.4456,  1.0081,\n",
      "         -1.1028, -2.0423, -0.6187, -1.4848, -0.1817, -1.5587,  0.2343, -0.5715,\n",
      "          0.6843,  0.4334, -0.0658,  0.3723, -0.0389,  0.5961, -0.7011,  3.7297,\n",
      "         -0.7646,  1.0899,  1.5763,  0.2432, -2.0390, -0.5935, -0.9231, -0.7742,\n",
      "         -0.3183,  0.0485,  0.0549,  0.7712,  0.4426, -0.2146,  0.8104, -0.2416,\n",
      "          0.3536,  0.0706]], grad_fn=<SliceBackward0>)\n",
      "Embeddings for the last two words:\n",
      "tensor([[ 0.6146, -0.4985, -1.5312,  0.6490, -0.4080,  0.7813, -0.9332,  0.5970,\n",
      "          0.1381,  0.2759,  1.2195, -0.0626, -0.5593,  2.5993,  1.7079,  1.1257,\n",
      "         -0.4921,  1.0145, -0.3823,  0.9068, -1.2434, -0.8899, -1.7885,  0.3117,\n",
      "          0.7528, -0.7442, -1.8633, -2.0981,  1.5663,  1.3877,  1.0717,  0.5510,\n",
      "         -1.2313,  1.8316,  0.4835,  0.8656,  0.2689,  0.1596,  0.1692,  0.7475,\n",
      "          0.5613,  0.9914,  1.4291, -0.5672, -0.2300,  2.4082, -0.7207,  0.7284,\n",
      "          1.4817, -0.0122],\n",
      "        [-0.4376,  0.6724, -0.2705,  1.6879,  0.8295, -1.1407,  1.7435,  0.7823,\n",
      "          0.1757,  1.9633,  0.0561,  0.0487, -0.0322, -0.8731,  1.9342,  0.2322,\n",
      "          0.9143, -0.3529, -1.3830, -0.3106, -0.8212,  2.6848, -0.0707, -2.7148,\n",
      "         -0.5522,  1.1801,  0.0570,  0.3223,  0.5293, -0.9904, -0.2698, -0.7164,\n",
      "          1.4516,  1.6161, -1.2917, -1.5764, -0.0345,  2.1818,  0.4721,  1.4332,\n",
      "         -0.5468,  1.1111, -0.7698,  1.5155, -0.5632,  1.2173,  0.4282, -1.0600,\n",
      "          0.1168,  0.0739]], grad_fn=<SliceBackward0>)\n",
      "Embedding for the word 'apple':\n",
      "tensor([-0.3587,  0.7732, -0.4567, -0.4940, -0.0791, -0.0285,  0.7708,  0.8347,\n",
      "        -1.5237,  1.0784, -1.8059,  0.6040, -1.5777,  0.1740,  1.4177,  0.9748,\n",
      "        -1.6658,  1.6476,  0.0658,  0.5767,  0.1060,  1.3074,  0.9831,  0.3646,\n",
      "         0.4986, -1.0046,  0.9260,  0.4735, -0.0531, -1.0035, -0.8946, -0.2291,\n",
      "        -0.5259, -0.3352,  0.6604,  1.0024, -0.1573, -2.2002,  0.3426, -0.3788,\n",
      "         0.7011,  1.0688, -0.5525,  0.4026, -0.0403,  0.0140,  0.5893, -0.0812,\n",
      "        -1.1798, -0.3190], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define a list of words (vocabulary)\n",
    "words = [\"apple\", \"banana\", \"orange\", \"pear\", \"peach\", \n",
    "         \"mango\", \"grape\", \"cherry\", \"berry\", \"melon\"]\n",
    "\n",
    "# Create mappings from word to index and index to word\n",
    "word2idx = {word: idx for idx, word in enumerate(words)}\n",
    "idx2word = {idx: word for idx, word in enumerate(words)}\n",
    "\n",
    "# Set embedding dimensions and create the embedding layer\n",
    "embedding_dim = 50\n",
    "embedding_layer = nn.Embedding(num_embeddings=len(words), embedding_dim=embedding_dim)\n",
    "\n",
    "# Get embeddings for all words in the vocabulary\n",
    "# (This will be a matrix of shape [vocab_size, embedding_dim])\n",
    "embeddings = embedding_layer(torch.arange(len(words)))\n",
    "# print the embedding for the first two words\n",
    "print(\"Embeddings for the first two words:\")\n",
    "print(embeddings[:2])\n",
    "# print the embedding for the last two words\n",
    "print(\"Embeddings for the last two words:\")\n",
    "print(embeddings[-2:])\n",
    "# print the embedding for the word \"apple\"\n",
    "print(\"Embedding for the word 'apple':\")\n",
    "print(embeddings[word2idx[\"apple\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(word, top_k=5):\n",
    "    \"\"\"Finds the top_k closest words to the input word using cosine similarity.\"\"\"\n",
    "    if word not in word2idx:\n",
    "        print(f\"Word '{word}' not found in vocabulary.\")\n",
    "        return []\n",
    "    \n",
    "    # Get the embedding for the input word\n",
    "    word_index = word2idx[word]\n",
    "    word_embedding = embeddings[word_index]\n",
    "    \n",
    "    # Normalize all embeddings to unit length\n",
    "    normalized_embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    normalized_word_embedding = F.normalize(word_embedding, p=2, dim=0)\n",
    "    \n",
    "    # Compute cosine similarities: dot product between normalized vectors\n",
    "    similarities = torch.matmul(normalized_embeddings, normalized_word_embedding)\n",
    "    \n",
    "    # Get the indices of the top (top_k+1) similar words (including the word itself)\n",
    "    top_values, top_indices = torch.topk(similarities, top_k + 1)\n",
    "    \n",
    "    results = []\n",
    "    for value, idx in zip(top_values, top_indices):\n",
    "        # Skip the word itself\n",
    "        if idx.item() == word_index:\n",
    "            continue\n",
    "        results.append((idx2word[idx.item()], value.item()))\n",
    "        if len(results) == top_k:\n",
    "            break\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words similar to 'banana':\n",
      "grape (cosine similarity: 0.1776)\n",
      "orange (cosine similarity: 0.0944)\n",
      "cherry (cosine similarity: 0.0777)\n",
      "berry (cosine similarity: 0.0662)\n",
      "peach (cosine similarity: 0.0537)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_word = \"banana\"\n",
    "closest_words = find_closest(input_word)\n",
    "\n",
    "if closest_words:\n",
    "    print(f\"Top {len(closest_words)} words similar to '{input_word}':\")\n",
    "    for word, similarity in closest_words:\n",
    "        print(f\"{word} (cosine similarity: {similarity:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
