{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest words via embeddings\n",
    "Here is the prompt to generate this code in ChatGPT\n",
    "Write a python program using pytorch to create embeddings for a list of words. Then given the input of one of the words find the 5 closest words to it. \n",
    "\n",
    "Question - Are the embeddings normalized? ie are they of length 1 ?\n",
    "\n",
    "## Exercise in Class\n",
    "Add a capability to embed 5 phrases containing up to 30 words, \n",
    "then given a query select the phrase that might answer that query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.nn' from '/home/codespace/.local/lib/python3.12/site-packages/torch/nn/__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings for the first two words:\n",
      "tensor([[ 0.3495,  0.0289,  0.2430, -1.0298,  0.9661, -0.2655, -0.2611,  0.5792,\n",
      "          2.0585,  0.4058, -1.1492, -0.5025,  0.3413,  0.2476, -0.3349, -0.8631,\n",
      "          0.4595,  0.8828,  1.0464,  0.8244, -0.5514,  1.0425,  1.5809, -0.0692,\n",
      "         -1.7990, -0.0602,  0.3207,  1.4453, -0.4037, -1.3846,  0.7818, -0.5465,\n",
      "         -0.7833,  0.0557,  0.5393,  0.1059, -0.9160, -1.7850, -0.1819,  1.7937,\n",
      "         -0.1177, -0.3234,  2.3065,  0.1515,  1.2161, -0.5575, -0.0381, -0.3167,\n",
      "         -0.0737, -0.3233],\n",
      "        [-1.5515,  1.2578,  2.1755,  0.5963,  1.1017, -0.4185, -0.1118,  0.8138,\n",
      "         -0.5492, -0.2341,  1.1922, -1.3007,  1.1381,  1.4680,  0.3003, -0.3165,\n",
      "         -0.3893, -0.0549, -1.0157,  0.0590, -0.8525,  0.8212,  0.2667, -0.1814,\n",
      "         -0.2477, -0.5772, -1.3342, -0.8670,  1.0715,  0.4388,  0.4143, -0.3818,\n",
      "          0.8707,  0.9559,  1.0466, -0.8718, -0.9823,  1.0048, -1.4688, -0.1079,\n",
      "          0.7671,  0.4526, -1.0224, -0.4266, -0.1653, -0.0284, -1.0911,  0.4494,\n",
      "          0.6780, -2.1361]], grad_fn=<SliceBackward0>)\n",
      "Embeddings for the last two words:\n",
      "tensor([[-0.6947, -1.1340, -0.8862,  0.2143, -0.8265,  0.1673, -1.5859,  0.8639,\n",
      "         -0.3782, -1.1689, -2.2697, -1.8071, -1.3185,  0.4630,  1.1436, -0.1331,\n",
      "          0.6534,  0.1599, -0.2861, -1.3196,  0.5103, -0.4344,  1.9851, -0.1805,\n",
      "          2.1125, -1.3697, -0.1164, -0.6615,  1.0652,  0.6142, -1.3016,  0.9361,\n",
      "          0.6285,  1.1003,  0.8067, -0.3807,  0.4752, -1.1837, -0.5455, -0.1544,\n",
      "         -0.6025, -0.2136, -0.3918,  1.0672, -0.6340, -0.7884, -0.8808,  0.8151,\n",
      "          2.0936, -2.5079],\n",
      "        [ 1.2067,  0.0500, -0.0920, -2.1541,  1.7866,  0.6035, -0.5588,  0.5260,\n",
      "          1.1435, -0.8159, -1.2960,  1.3075,  0.4928,  0.3194,  0.7302, -1.2231,\n",
      "          0.7845, -0.9863, -1.2103, -0.7413,  0.5113,  0.2165, -0.7255, -1.3928,\n",
      "         -3.5634,  0.0782,  0.6758, -0.7294, -1.0473,  0.3336, -0.5081, -0.3682,\n",
      "         -1.6342,  0.2844, -0.1378,  0.1991, -0.5183,  0.8545,  0.3721,  0.3098,\n",
      "          0.7580,  0.8574, -0.4620,  1.1652, -0.0825,  0.3191, -1.1167, -1.3025,\n",
      "         -0.1056, -1.2679]], grad_fn=<SliceBackward0>)\n",
      "Embedding for the word 'apple':\n",
      "tensor([ 0.3495,  0.0289,  0.2430, -1.0298,  0.9661, -0.2655, -0.2611,  0.5792,\n",
      "         2.0585,  0.4058, -1.1492, -0.5025,  0.3413,  0.2476, -0.3349, -0.8631,\n",
      "         0.4595,  0.8828,  1.0464,  0.8244, -0.5514,  1.0425,  1.5809, -0.0692,\n",
      "        -1.7990, -0.0602,  0.3207,  1.4453, -0.4037, -1.3846,  0.7818, -0.5465,\n",
      "        -0.7833,  0.0557,  0.5393,  0.1059, -0.9160, -1.7850, -0.1819,  1.7937,\n",
      "        -0.1177, -0.3234,  2.3065,  0.1515,  1.2161, -0.5575, -0.0381, -0.3167,\n",
      "        -0.0737, -0.3233], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define a list of words (vocabulary)\n",
    "words = [\"apple\", \"banana\", \"orange\", \"pear\", \"peach\", \n",
    "         \"mango\", \"grape\", \"cherry\", \"berry\", \"robot\"]\n",
    "\n",
    "# Create mappings from word to index and index to word\n",
    "word2idx = {word: idx for idx, word in enumerate(words)}\n",
    "idx2word = {idx: word for idx, word in enumerate(words)}\n",
    "\n",
    "# Set embedding dimensions and create the embedding layer\n",
    "embedding_dim = 50\n",
    "embedding_layer = nn.Embedding(num_embeddings=len(words), embedding_dim=embedding_dim)\n",
    "\n",
    "# Get embeddings for all words in the vocabulary\n",
    "# (This will be a matrix of shape [vocab_size, embedding_dim])\n",
    "embeddings = embedding_layer(torch.arange(len(words)))\n",
    "# print the embedding for the first two words\n",
    "print(\"Embeddings for the first two words:\")\n",
    "print(embeddings[:2])\n",
    "# print the embedding for the last two words\n",
    "print(\"Embeddings for the last two words:\")\n",
    "print(embeddings[-2:])\n",
    "# print the embedding for the word \"apple\"\n",
    "print(\"Embedding for the word 'apple':\")\n",
    "print(embeddings[word2idx[\"apple\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(word, top_k=5):\n",
    "    \"\"\"Finds the top_k closest words to the input word using cosine similarity.\"\"\"\n",
    "    if word not in word2idx:\n",
    "        print(f\"Word '{word}' not found in vocabulary.\")\n",
    "        return []\n",
    "    \n",
    "    # Get the embedding for the input word\n",
    "    word_index = word2idx[word]\n",
    "    word_embedding = embeddings[word_index]\n",
    "    \n",
    "    # Normalize all embeddings to unit length\n",
    "    normalized_embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    normalized_word_embedding = F.normalize(word_embedding, p=2, dim=0)\n",
    "    \n",
    "    # Compute cosine similarities: dot product between normalized vectors\n",
    "    similarities = torch.matmul(normalized_embeddings, normalized_word_embedding)\n",
    "    \n",
    "    # Get the indices of the top (top_k+1) similar words (including the word itself)\n",
    "    top_values, top_indices = torch.topk(similarities, top_k + 1)\n",
    "    \n",
    "    results = []\n",
    "    for value, idx in zip(top_values, top_indices):\n",
    "        # Skip the word itself\n",
    "        if idx.item() == word_index:\n",
    "            continue\n",
    "        results.append((idx2word[idx.item()], value.item()))\n",
    "        if len(results) == top_k:\n",
    "            break\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words similar to 'banana':\n",
      "berry (cosine similarity: 0.1695)\n",
      "cherry (cosine similarity: 0.1219)\n",
      "orange (cosine similarity: 0.0211)\n",
      "robot (cosine similarity: 0.0143)\n",
      "mango (cosine similarity: -0.0414)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_word = \"banana\"\n",
    "closest_words = find_closest(input_word)\n",
    "\n",
    "if closest_words:\n",
    "    print(f\"Top {len(closest_words)} words similar to '{input_word}':\")\n",
    "    for word, similarity in closest_words:\n",
    "        print(f\"{word} (cosine similarity: {similarity:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
