{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Using NN and BackProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Sample data (x values and corresponding y values)\n",
    "x_data = np.array([0, 1, 2, 3, 4, 5])  # Input x values\n",
    "y_data = np.array([1, 3, 5, 7, 9, 11])  # Output y values (y = 2x + 1)\n",
    "\n",
    "# Initialize weights randomly\n",
    "w1 = np.random.randn()  # Weight for x\n",
    "w2 = np.random.randn()  # Bias weight\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.01\n",
    "\n",
    "# Number of training iterations\n",
    "epochs = 100\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_loss = 0  # Accumulate loss over all training samples\n",
    "    \n",
    "    grad_w1_sum = 0  # Accumulate gradient for w1\n",
    "    grad_w2_sum = 0  # Accumulate gradient for w2\n",
    "\n",
    "    for i in range(len(x_data)):\n",
    "        x = x_data[i]\n",
    "        y_true = y_data[i]\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = w1 * x + w2  # Compute predicted value\n",
    "\n",
    "        # Compute loss (Mean Squared Error)\n",
    "        loss = (y_pred - y_true) ** 2\n",
    "        total_loss += loss  # Sum loss over all data points\n",
    "\n",
    "        # Compute gradients (Backpropagation)\n",
    "        grad_w1 = 2 * (y_pred - y_true) * x  # Derivative of loss w.r.t w1\n",
    "        grad_w2 = 2 * (y_pred - y_true) * 1  # Derivative of loss w.r.t w2 (bias)\n",
    "\n",
    "        # Accumulate gradients\n",
    "        grad_w1_sum += grad_w1\n",
    "        grad_w2_sum += grad_w2\n",
    "\n",
    "    # Update weights using the **average gradient** over all samples\n",
    "    w1 -= lr * (grad_w1_sum / len(x_data))\n",
    "    w2 -= lr * (grad_w2_sum / len(x_data))\n",
    "\n",
    "    # Store average loss per epoch\n",
    "    loss_history.append(total_loss / len(x_data))\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(x_data):.4f}, w1: {w1:.4f}, w2: {w2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(range(epochs), loss_history, label=\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
